{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 数据可视化\n",
    "# import matplotlib.pyplot as plt\n",
    "# df_data['sentence_len'].hist(bins=100)\n",
    "# plt.xlim(0, 100)\n",
    "# plt.xlabel('sentence_length')\n",
    "# plt.ylabel('sentence_num')\n",
    "# plt.title('Distribution of the Length of Sentence')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from itertools import chain\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# filename = './data/sentiment_word_tagging_train.csv'\n",
    "def word_to_index(sentence, flag, tokenizer):\n",
    "    res = []\n",
    "    if flag == 'content':\n",
    "        tmp = tokenizer.texts_to_sequences(sentence)\n",
    "        for i in tmp:\n",
    "            if i:\n",
    "                res.append(i[0])\n",
    "    else:\n",
    "        tag_dict = {'B':1, 'M':2, 'E':3, 'S':4, 'N':5}\n",
    "        for word in sentence:\n",
    "            res.append(tag_dict[word])\n",
    "    return [res]\n",
    "\n",
    "def get_Xy(sentence):\n",
    "    \"\"\"将 sentence 处理成 [word1, w2, ..wn], [tag1, t2, ...tn]\"\"\"\n",
    "    sentence = sentence.replace(\"//\", '$/')\n",
    "    words_tags = re.findall('(.)/(.)', sentence)\n",
    "    if words_tags:\n",
    "        words_tags = np.asarray(words_tags)\n",
    "        words = words_tags[:, 0]\n",
    "        tags = words_tags[:, 1]\n",
    "        return words, tags # 所有的字和tag分别存为 data / label\n",
    "    return None\n",
    "\n",
    "def initTokenizer():\n",
    "    df = pd.read_csv('./data/allwords.csv', dtype=np.str, header=None)\n",
    "    all_words = df[0].values\n",
    "    tokenizer = Tokenizer(lower=False)\n",
    "    tokenizer.fit_on_texts(all_words)\n",
    "    return tokenizer\n",
    "zy = {}\n",
    "def initTranspos(transposematrix, labels):\n",
    "    for label in labels:\n",
    "        for i in range(1, len(label)):\n",
    "            status = label[i-1] + label[i]\n",
    "            if transposematrix.get(status, -1) == -1:\n",
    "                transposematrix[status] = 1\n",
    "            else:\n",
    "                transposematrix[status] = transposematrix[status] + 1\n",
    "\n",
    "def generate_data(filename):\n",
    "    raw_data = pd.read_csv(filename, header=None, delimiter='\\t')\n",
    "    s = ''\n",
    "    for index, row in raw_data.iterrows():\n",
    "        if index != 0:\n",
    "            s = s + ' '\n",
    "        s = s + row.values[0]\n",
    "    sentences = re.split(u'[，。！？、‘’“”]/[BMENS]', s)\n",
    "    datas = []\n",
    "    labels = []\n",
    "    for sentence in iter(sentences):\n",
    "        res = get_Xy(sentence)\n",
    "        if res:\n",
    "            datas.append(res[0])\n",
    "            labels.append(res[1])\n",
    "#     initTranspos(zy, labels)\n",
    "    df_data = pd.DataFrame({'words': datas, 'tags': labels}, index=range(len(datas)))\n",
    "    #　句子长度\n",
    "    df_data['sentence_len'] = df_data['words'].apply(lambda words: len(words))\n",
    "\n",
    "\n",
    "    tokenizer = initTokenizer()\n",
    "    df_data['X'] = df_data['words'].apply(word_to_index, args = ['content', tokenizer])\n",
    "    df_data['Y'] = df_data['tags'].apply(word_to_index, args = ['sentiment', tokenizer])\n",
    "    print(\"finish word_to_index\")\n",
    "    \n",
    "    maxlen = 40\n",
    "    df_data['X'] = df_data['X'].apply(pad_sequences, args=[maxlen, 'int32', 'post'])\n",
    "    df_data['Y'] = df_data['Y'].apply(pad_sequences, args=[maxlen, 'int32', 'post'])\n",
    "    \n",
    "    X = np.asarray(list(df_data['X'].values))\n",
    "    y = np.asarray(list(df_data['Y'].values))\n",
    "    X = X.reshape(X.shape[0], X.shape[2])\n",
    "    y = y.reshape(y.shape[0], y.shape[2])\n",
    "    \n",
    "    #将标签向量one-hot\n",
    "    def getY(y):\n",
    "        res = []\n",
    "        for row in y:\n",
    "            tmp = []\n",
    "            for col in row:\n",
    "                tmp.append(np_utils.to_categorical(col, 6))\n",
    "            res.append(tmp)\n",
    "        return np.array(res)\n",
    "    y = getY(y)\n",
    "    y = y.reshape(-1, 40, 6)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish word_to_index\n"
     ]
    }
   ],
   "source": [
    "X, y = generate_data('./data/sentiment_word_tagging_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        ..., \n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        ..., \n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        ..., \n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "       ..., \n",
       "       [[ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        ..., \n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  1.,  0.,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  1.],\n",
       "        ..., \n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.,  0.,  0.]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 40\n",
    "word_size = 128\n",
    "from keras.layers import Dense, Embedding, LSTM, TimeDistributed, Input, Bidirectional\n",
    "from keras.models import Model\n",
    "\n",
    "sequence = Input(shape=(maxlen,), dtype='int32')\n",
    "embedded = Embedding(len(all_words)+1, word_size, input_length=maxlen, mask_zero=True)(sequence)\n",
    "blstm = Bidirectional(LSTM(64, return_sequences=True), merge_mode='sum')(embedded)\n",
    "output = TimeDistributed(Dense(6, activation='softmax'))(blstm)\n",
    "model = Model(input=sequence, output=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 1024\n",
    "history = model.fit(X, y, batch_size=batch_size, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5728/5728 [==============================] - 3s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.25671975493597587, 0.91665562338003237]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('./model/sentiment_model.hdf5')\n",
    "model.predict()\n",
    "\n",
    "\n",
    "x1\n",
    "\n",
    "x1 = np.array([X[0]])\n",
    "\n",
    "y1 = y[0]\n",
    "predict = model.predict(x1)\n",
    "\n",
    "predict = predict.reshape(40, 6)\n",
    "\n",
    "\n",
    "predict[0:10]\n",
    "\n",
    "res = []\n",
    "for row in predict:\n",
    "    res.append(np.argmax(row))\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "data = pd.read_csv('./data/test.csv', delimiter='\\t')\n",
    "content = data.iloc[0][\"content\"]\n",
    "content + '。'\n",
    "model = load_model('./model/sentiment_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 情感词转移矩阵\n",
    "zy = {'BE': 0.8388004045000632,\n",
    " 'BM': 0.1611995954999368,\n",
    " 'EB': 0.08903315557616508,\n",
    " 'EN': 0.990513604698914,\n",
    " 'ES': -0.07954676027507912,\n",
    " 'ME': 0.7658009307911725,\n",
    " 'MM': 0.2341990692088275,\n",
    " 'NB': 0.0655631997557599,\n",
    " 'NN': 0.9842186311840461,\n",
    " 'NS': -0.04978183093980593,\n",
    " 'SB': 0.05394649568870219,\n",
    " 'SN': 0.00023000846530342705,\n",
    " 'SS': 0.9458234958459945\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = zy\n",
    "zy['BE'] = A['BE'] / (A['BE'] + A['BM'])\n",
    "zy['BM'] = 1 - zy['BE']\n",
    "\n",
    "zy['EB'] = A['EB'] / (A['EN'] + A['ES'] + A['EB'])\n",
    "zy['EN'] = A['EN'] / (A['EN'] + A['ES'] + A['EB'])\n",
    "zy['ES'] = 1 - zy['EB'] - zy['EN']\n",
    "\n",
    "zy['ME'] = A['ME'] / (A['ME'] + A['MM'])\n",
    "zy['MM'] = 1 - zy['ME']\n",
    "\n",
    "zy['NB'] = A['NB'] / (A['NN'] + A['NS'] + A['NB'])\n",
    "zy['NN'] = A['NN'] / (A['NN'] + A['NS'] + A['NB'])\n",
    "zy['NS'] = 1 - zy['NB'] - zy['NN']\n",
    "\n",
    "zy['SB'] = A['SB'] / (A['SN'] + A['SS'] + A['SB'])\n",
    "zy['SN'] = A['NN'] / (A['SN'] + A['SS'] + A['SB'])\n",
    "zy['SS'] = 1 - zy['SB'] - zy['SN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.97639133e-06   2.00652881e-04   1.99369359e-04   7.59826333e-04\n",
      "    1.44626872e-04   9.98691499e-01]\n",
      " [  3.40076014e-07   8.66002170e-04   1.12468369e-04   4.29953798e-05\n",
      "    3.07019873e-05   9.98947442e-01]\n",
      " [  2.43994577e-06   1.06700500e-02   3.10417404e-03   2.01119389e-03\n",
      "    3.97963775e-03   9.80232477e-01]\n",
      " [  1.87809121e-06   3.40575904e-01   1.18051685e-01   2.02678214e-03\n",
      "    5.32808597e-04   5.38810909e-01]\n",
      " [  2.29584166e-05   5.37308026e-03   7.65544176e-01   1.96815282e-02\n",
      "    3.98820906e-04   2.08979353e-01]\n",
      " [  1.50923215e-05   2.73587078e-01   5.62995672e-01   3.16475257e-02\n",
      "    2.18529356e-04   1.31536067e-01]\n",
      " [  4.29033171e-05   1.93575269e-03   2.86455341e-02   8.73806596e-01\n",
      "    2.41114351e-04   9.53281447e-02]]\n",
      "['我', '觉', '得', '不', '是', '正', '品']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "zy = {i:np.log(zy[i]) for i in zy.keys()}\n",
    "def viterbi(nodes):\n",
    "    \"\"\"\n",
    "    维特比译码：除了第一层以外，每一层有4个节点。\n",
    "    计算当前层（第一层不需要计算）四个节点的最短路径：\n",
    "       对于本层的每一个节点，计算出路径来自上一层的各个节点的新的路径长度（概率）。保留最大值（最短路径）。\n",
    "       上一层每个节点的路径保存在 paths 中。计算本层的时候，先用paths_ 暂存，然后把本层的最大路径保存到 paths 中。\n",
    "       paths 采用字典的形式保存（路径：路径长度）。\n",
    "       一直计算到最后一层，得到四条路径，将长度最短（概率值最大的路径返回）\n",
    "    \"\"\"\n",
    "    # 第一层，三个节点\n",
    "    paths = {'B': nodes[0]['B'], 'S':nodes[0]['S'], 'N':nodes[0][\"N\"]} \n",
    "    for layer in range(1, len(nodes)):  # 后面的每一层\n",
    "        paths_ = paths.copy()  # 先保存上一层的路径\n",
    "        # node_now 为本层节点， node_last 为上层节点\n",
    "        paths = {}  # 清空 path \n",
    "        for node_now in nodes[layer].keys():\n",
    "            # 对于本层的每个节点，找出最短路径\n",
    "            sub_paths = {} \n",
    "            # 上一层的每个节点到本层节点的连接\n",
    "            for path_last in paths_.keys():\n",
    "                if path_last[-1] + node_now in zy.keys(): # 若转移概率不为 0 \n",
    "                    sub_paths[path_last + node_now] = paths_[path_last] + nodes[layer][node_now] + zy[path_last[-1] + node_now]\n",
    "            # 最短路径,即概率最大的那个\n",
    "            sr_subpaths = pd.Series(sub_paths)\n",
    "            sr_subpaths = sr_subpaths.sort_values()  # 升序排序\n",
    "            node_subpath = sr_subpaths.index[-1]  # 最短路径\n",
    "            node_value = sr_subpaths[-1]   # 最短路径对应的值\n",
    "            # 把 node_now 的最短路径添加到 paths 中\n",
    "            paths[node_subpath] = node_value\n",
    "    # 所有层求完后，找出最后一层中各个节点的路径最短的路径\n",
    "    sr_paths = pd.Series(paths)\n",
    "    sr_paths = sr_paths.sort_values()  # 按照升序排序\n",
    "    return sr_paths.index[-1]  # 返回最短路径（概率值最大的路径）\n",
    "\n",
    "def simple_cut(s, tokenizer):\n",
    "    if s:\n",
    "        tmp = sent_to_vec(s, tokenizer)[0]\n",
    "        tmp.extend([0]*(40-len(s)))\n",
    "        tmp = np.array(tmp)\n",
    "        tmp = tmp.reshape(-1, 40)\n",
    "        r = model.predict(tmp, verbose=False)[0][:len(s)]\n",
    "        print(r)\n",
    "        r = np.log(r)\n",
    "        nodes = [dict(zip(['B','M','E','S', 'N'], i[1:])) for i in r]\n",
    "        t = viterbi(nodes)\n",
    "        words = []\n",
    "        for i in range(len(s)):\n",
    "            if t[i] in ['S', 'B', 'N']:\n",
    "                words.append(s[i])\n",
    "            else:\n",
    "                words[-1] += s[i]\n",
    "        return words\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# not_cuts = re.compile(u'[。，、？！\\.\\?,!]')\n",
    "# def cut_word(s):\n",
    "#     result = []\n",
    "#     j = 0\n",
    "#     for i in not_cuts.finditer(s):\n",
    "#         result.extend(simple_cut(s[j:i.start()]))\n",
    "#         result.append(s[i.start():i.end()])\n",
    "#         j = i.end()\n",
    "#     result.extend(simple_cut(s[j:]))\n",
    "#     return result\n",
    "\n",
    "def sent_to_vec(s, tokenizer):\n",
    "    res = word_to_index(s, \"content\", tokenizer)\n",
    "    for item in res:\n",
    "        if len(item) == 0:\n",
    "            item.append(0)\n",
    "    return res\n",
    "token = initTokenizer()\n",
    "print(simple_cut('我觉得不是正品', token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50784/50796 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24350443599153498, 0.92025650792332536]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
